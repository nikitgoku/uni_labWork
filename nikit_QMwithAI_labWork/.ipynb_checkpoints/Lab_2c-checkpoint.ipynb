{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> CS824 - Material for Lab 2c (2021) </div>\n",
    "\n",
    "## Central Limit Theorem (CLT) demo - linked to Week 2 lab submission\n",
    "\n",
    "We have noted that the CLT states that, \"the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger - **irrespective** of the underlying distribution from which the data, that these sample means estimate, have been taken.\"\n",
    "\n",
    "Let's try this out for a situation where we **_know_** that we are NOT working with a Normal distribution - the case of a 6-sided die where we are talking about a (discrete) *Uniform* distribution with each of the values 1 to 6 being equally likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our 'virtual' die (\"die\" is the singular of \"dice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's start off throwing the die 10 times - our 'expected value' over a large number of rolls will \n",
    "be 3.5 but we expect a fair bit of variation due to random chance.\n",
    "\n",
    "We use the 'randint' function which is a *discrete* uniform distribution returning only intergers, here [1-6].\n",
    "If we used \"random.uniform(0,7)\" we should get an almost identical average, but that would be like rolling a\n",
    "die with an infinite number of 'sides' (between 1 and 6)!!\n",
    "\n",
    "I am going to set a random seed, so that my commentary below matches the specific outcomes discussed.\n",
    "If you feel this is 'cheating' feel free to alter (or remove) that line once you have read/run, to make sure \n",
    "that the same behaviour is observed, irrespective of the random seed choosen (include none selected).\n",
    "'''\n",
    "\n",
    "seed(17)\n",
    "\n",
    "# Note: 'randint' (and indeed 'unifrom') are so-called \"half-open\" intervals, i.e. they include the lower \n",
    "# bound (here 1), but exclude the upper bound. \n",
    "# (There is actually another option, \"random_integers{1,6)\" which uses a closed interval.}\n",
    "\n",
    "die_10_times = np.random.randint(1,7,10)\n",
    "\n",
    "print(die_10_times)\n",
    "print(np.average(die_10_times));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see the results of each of the 10 rolls of the die and their average = 3.2 (not that far away from the 'expected' value of 3.5, but also not exactly that close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose my seed above (17) at 'random' and happened to get the value of 3.2. \n",
    "\n",
    "See what would have happened if I had happened to choose:\n",
    " - **seed = 19**   (You might have thought I was 'cheating'!)\n",
    " - or **seed = 15**   (Not very close to the expected value).\n",
    " \n",
    "However, the point is that 10 throws is not that many for a variable that can take on any one of six values, so we really need to run a longer experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What happens in we roll a die 100, 1,000 or 10,000 times?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So in addition to 10 times lets try rolling the die from 100 up to 10,000 times to see what happens...  \n",
    "# In this case we will *not* want to print out all the throw values!  Better just to look at the averages.\n",
    "\n",
    "die_10_times = np.random.randint(1,7,10)\n",
    "die_100_times = np.random.randint(1,7,100)\n",
    "die_1000_times = np.random.randint(1,7,1000)\n",
    "die_10000_times = np.random.randint(1,7,10000)\n",
    "\n",
    "print(np.average(die_10_times))\n",
    "print(np.average(die_100_times))\n",
    "print(np.average(die_1000_times))\n",
    "print(np.average(die_10000_times));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before thinking about the trends in these results, a couple of things to note:\n",
    " - the value that you get for the 10 throws case is NOT the same as you got in the first experiment, can you explain why?\n",
    " - I have not carried out any nice formatting on the outputs but you can see that they are typically given to 1, 2, 3 and 4 decimal places. Think about why this is the case. In actual fact the numbers we are generating here in the averages are NOT continuous values, rather they are discrete (just not on the integer number line).\n",
    " \n",
    "For example, in the '10 dice thrown' example, because we are taking 10 values and then dividing the total obtained by 10, the list of possible values must belong to the set [1.0, 1.1, 1.2, 1.3, ... 5.9, 6.0] - i.e. this is one of 60 **discrete** values. In the case of 10,000 throws it may look like we have a 'continuous' value - e.g. \"3.5134\" - but again this is in fact one value from a possible set of 60,000 discrete values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for those who *need* to see 'raw' values...  you can take a look at the 1,000 rolls case!!\n",
    "\n",
    "print(die_1000_times);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the estimate tends to converge closer to the **expected value of 3.5** as we take more and more samples (make more rolls and take their average). If you ran the code above a few times you may have seen that this was not always the case, particularly with the larger numbers of throws, i.e. 10,000 is often not that much better, and sometimes worse, than 1,000 throws. However, what we have considered so far is the **Law of Large Numbers** at work, *NOT* the Central Limit Theoerm.\n",
    "\n",
    "### Central Limit Theorem\n",
    "\n",
    "To explore the CLT we need to look at the **sampling distribution** of the sample means, which should approach a *Normal* distribution as the number of samples included gets larger (irrespective of the underlying distribution).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try 1000 simulations of our 10 die roll case (for which know that each estimate can be quite far away\n",
    "# from the expected value - e.g. 3.2 and many worse in the examples above)\n",
    "\n",
    "seed(17)\n",
    "n = 1000\n",
    "\n",
    "# Create an empty array into which we will insert each of the 1000 averages for 10 rolls\n",
    "avg_tens_estimates = []\n",
    "\n",
    "# For each simulation('experiment'), roll the die 10 times and capture the average value\n",
    "for i in range(1,n):\n",
    "    a = np.random.randint(1,7,10)\n",
    "    avg_tens_estimates.append(np.average(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could take a look at the first 5 averages of these 1000 10-die roll experiments\n",
    "avg_tens_estimates[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB:\n",
    "You can see that the first entry in this list is our old friend \"3.2\" from the earlier simulation - due to the fact that we have again used the \"seed(17)\" statement to prime the values from our random number generator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The moment of truth... what does the distribution look like?\n",
    "\n",
    "If the CLT is valid, then despite the fact that none of these individual estimates is that close to 3.5 (and that they each come from a Uniform distribution), the overall 'shape' of these 1,000 sample means taken together should start to resemble a Normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribtion of these sample means\n",
    "\n",
    "plt.hist(avg_tens_estimates)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, yes this is roughly symmertical around 3.5 and seems approximately 'Normal', but the values around 3.0-3.2 and 3.8-4.0 seem to have a higher absolute value than the exact 'expected' value of 3.5 \n",
    "\n",
    "We could look at 10,000 experiments...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try 10,000 simulations of our 10 die roll case\n",
    "\n",
    "seed(12)\n",
    "n = 10000\n",
    "\n",
    "# Create an empty array into which we will insert each of the 1000 averages for 10 rolls\n",
    "avg_tens_estimates2 = []\n",
    "\n",
    "# For each simulation('experiment'), roll the die 10 times and capture the average value\n",
    "for i in range(1,n):\n",
    "    a = np.random.randint(1,7,10)\n",
    "    avg_tens_estimates2.append(np.average(a))\n",
    "    \n",
    "plt.hist(avg_tens_estimates2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well maybe a bit more 'Normal' but remember that each trial returns a limited set of values (each one is divided by 10 and as such is only different by at most 0.x - i.e. one decimal place).\n",
    "\n",
    "Thus we should really call on the Law of Large numbers as well as CLT to get a 'nicer' graphical output. \n",
    "\n",
    "Let's stick to the 10,000 experiments, but within each let's explore 100 rolls of the die in each experiment, rather than just 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try 10,000 simulations of a 100 die roll\n",
    "\n",
    "seed(21)\n",
    "n = 10000\n",
    "\n",
    "# Create an empty array into which we will insert each of the 1000 averages for 10 rolls\n",
    "avg_hundreds_estimates = []\n",
    "\n",
    "# For each simulation('experiment'), roll the die 100 times and capture the average value\n",
    "for i in range(1,n):\n",
    "    a = np.random.randint(1,7,100)\n",
    "    avg_hundreds_estimates.append(np.average(a))\n",
    "    \n",
    "plt.hist(avg_hundreds_estimates)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Pictures' can be powerful\n",
    "\n",
    "OK, so hopefully you now get the picture...  You should note that the actual scale on the x-axis has automatically been altered (as have the widths of the 'bins'). It is usually fine to let the plotting algorithm make selections for elements such as axis limits and bin-width (at least initially). However, particularly if you want to compare between scenarios it is sometimes wise to manually 'fix' these elements so as to ensure that you are making comparisons over simular outputs.\n",
    "\n",
    "If you have time, look at the \"plt.hist\" function in *matplotlib* and work with the same axis and bin width for the 'tens' and 'hundreds' case, and see whether you feel it makes the point more strongly...  I think it does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't always trust your eyes\n",
    "\n",
    "However, despite the fact that I have been emphasising the importance of EDA and 'taking a look' (wherever possible) at the data, there are also cases where graphical output is misleading or inadequate. For example, rather than looking at the histograms of these experiments, we could run a more formal statistical test of normality and see what that revealed about the various options we have so far tried...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we conatenate the values from these two distributions, one with mean 0 and the other with mean 2 (both\n",
    "# with SD = 1,) we would not expect a 'normal' outcome - more likely some sort of bi-modal distribution.\n",
    "# We are only going to use the p-value part of what is returned from the function - i.e. not the 'statistic' itself\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "stat, p_estm1 = stats.normaltest(avg_tens_estimates)\n",
    "stat, p_estm2 = stats.normaltest(avg_tens_estimates2)\n",
    "stat, p_estm3 = stats.normaltest(avg_hundreds_estimates)\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "print(\"p-value for normality on the 1,000 experiments with a 10-die role = {:.4f}\".format(p_estm1))\n",
    "if p_estm1 < alpha:                        # null hypothesis: sample comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected, i.e. this sample likely did NOT come from a Normal distribution\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected, i.e. this sample likely comes from a Normal distribution\")\n",
    "\n",
    "print()\n",
    "print(\"p-value for normality on the 10,000 experiments with a 10-die role = {:.4f}\".format(p_estm2))\n",
    "if p_estm2 < alpha:                        # null hypothesis: sample comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected, i.e. this sample likely did NOT come from a Normal distribution\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected, i.e. this sample likely comes from a Normal distribution\")\n",
    "\n",
    "print()    \n",
    "print(\"p-value for normality on the 10,000 experiments with a 100-die role = {:.4f}\".format(p_estm3))\n",
    "if p_estm3 < alpha:                        # null hypothesis: sample comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected, i.e. this sample likely did NOT come from a Normal distribution\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected, i.e. this sample likely comes from a Normal distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note of caution\n",
    "\n",
    "So for this set of examples ALL of the cases appear to meet the assumption that they came from a Normal distribution. (Well, strictly speaking, we can only say that we cannot reject the null hypothesis that the likely did not come from a Normal distribution!)\n",
    "\n",
    "However, you may wish to go back to the \"avg_tens_estimates2\" case and **alter** the `seed` from 12 to 21. If you do this you should find that the second set of experiments does NOT conform to the Normal case. i.e. the p-value in that text is <0.00001 and as such the null hypothesis CAN be rejected - i.e. the test appears to suggest that the data did **not** come from a Normal distribution, despite the CLT!!  You can see visually that this choice of random seed does indeed seem to lead to a set of estimates that demonstates a much stronger right-hand skew. Such can be the challenges of trying to make generalisations when working with random samples!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab submission for Week2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I would like you to do within your small group is look at the Law of Large Numbers and the Central Limit Theorem in the context of the **'pi estimation'** code that you developed in Week 1.\n",
    "\n",
    "You may already have looked at this a bit (at least the Law of Large Numbers) in that you noticed that as a general rule your estimates tended to get better as you increased the number of 'darts'. However, you may have seen that this was not always the case. Also, even with just 100 or certainly 1,000 darts you should be able to get some interesting results if you take the values of each experiment as a sample mean for pi and then look at the distribution of those sample means.\n",
    "\n",
    "I am not going to be prescriptive in terms of how you tackle this submission. However, I would like to see code and outputs that demonstrate the fact that you have absorbed the ideas in the exercises above and have then applied those to your own 'pi estimation' experiments. \n",
    "\n",
    "Here are a few things that I will be looking for:\n",
    "\n",
    " - a decent amount of **comments** within your code so that I can understand the steps you took;\n",
    " - some **graphical outputs** that let me easily see the overall effects of different choiced you have made;\n",
    " - some **comparisons** to make a point. e.g. Let's assume you had 1M darts to throw, what difference does it make if you throw all 1M of these at the board and get an estimate, compares to throwing 1,000 darts in 1,000 different experiments (i.e. the same overall number of darts) and looking at the mean of these 1,000 estimates for pi?;\n",
    " - at least one example of using a **normality test** (i.e. rather than just relying on 'eye-balled' the data).\n",
    "\n",
    "### BONUS Marks\n",
    "Depending on how 'Pythonic' you made your original pi estimation function, you may or may not need to re-implement things a bit for this part of the exercise. You should create 'for loop' and 'numpy does the heavy lifting' versions of your pi estimator function and then put these within a 'timer' setting to compare their relative performance. (You will likely have to run, say 1M simulations to see a meaningful difference - i.e. at a few 1,000 simulations the times may all be down in the sub 1 second time-frame.) \n",
    "\n",
    "\n",
    "I will be setting up the submission link so that you only need to make ONE file submission per group. (However, don't forget to include the Group_name as part of the filename of your submitted file.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
